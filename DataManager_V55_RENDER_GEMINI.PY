#!/usr/bin/env python3
import os, sys, time, zipfile, requests, pandas as pd, numpy as np, joblib, warnings, random, threading
from datetime import datetime, timedelta
from io import BytesIO
from http.server import HTTPServer, SimpleHTTPRequestHandler
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

warnings.filterwarnings("ignore")
sys.stdout.reconfigure(line_buffering=True)

# ============================================================
# CONFIGURAÃ‡ÃƒO IMPECÃVEL (DNA V51 + PERSISTÃŠNCIA RENDER)
# ============================================================
SYMBOL = "PENDLEUSDT"
START_DT = datetime(2025, 1, 1, 0, 0, 0)
END_DT = datetime(2025, 12, 31, 23, 59, 59)

BASE_DISK = "/data" if os.path.exists("/data") else "."
FOLDER_NAME = f"pendle_agg_{START_DT.strftime('%Y%m%d')}_a_{END_DT.strftime('%Y%m%d')}"
OUT_DIR = os.path.join(BASE_DISK, FOLDER_NAME)
MODELOS_DIR = os.path.join(OUT_DIR, "modelos_v27")
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(MODELOS_DIR, exist_ok=True)

CSV_15M_FINAL = os.path.join(OUT_DIR, f"{SYMBOL}_15m.csv")
ZIP_CSV_PATH = os.path.join(BASE_DISK, f"{FOLDER_NAME}_CSVs.zip")
ZIP_PKL_PATH = os.path.join(BASE_DISK, f"{FOLDER_NAME}_PKLs.zip")

BASE_URL = "https://data.binance.vision/data/futures/um/daily/aggTrades"
WHALE_THRESHOLD = 500

def upload_catbox(file_path):
    url = "https://catbox.moe/user/api.php"
    if not os.path.exists(file_path): return "INEXISTENTE"
    try:
        with open(file_path, 'rb') as f:
            data = {'reqtype': 'fileupload', 'userhash': ''}
            files = {'fileToUpload': f}
            response = requests.post(url, data=data, files=files, timeout=60)
        return response.text
    except: return "ERRO"

class DownloadHandler(SimpleHTTPRequestHandler):
    def translate_path(self, path):
        return super().translate_path(path)

def start_server(port=8080):
    server = HTTPServer(('0.0.0.0', port), DownloadHandler)
    print(f"ðŸŒ Servidor na porta {port}", flush=True)
    server.serve_forever()

def process_aggtrades_to_15m(df_raw):
    df = df_raw.copy()
    df.columns = ['id', 'price', 'qty', 'first_id', 'last_id', 'ts', 'is_buyer_maker']
    df['val_usd'] = df['price'] * df['qty']
    df['ts_bucket'] = (df['ts'] // (15 * 60 * 1000)) * (15 * 60 * 1000)
    df['is_buy'] = df['is_buyer_maker'] == False
    
    grouped = df.groupby('ts_bucket')
    res = pd.DataFrame()
    res['open'] = grouped['price'].first()
    res['high'] = grouped['price'].max()
    res['low'] = grouped['price'].min()
    res['close'] = grouped['price'].last()
    res['volume'] = grouped['qty'].sum()
    res['quote_volume'] = grouped['val_usd'].sum()
    res['trades'] = grouped['id'].count()
    
    # taker_buy_base = VOLUME TOTAL (Sardinha + Baleia)
    res['taker_buy_base'] = df[df['is_buy']].groupby('ts_bucket')['qty'].sum()
    res['taker_buy_quote'] = df[df['is_buy']].groupby('ts_bucket')['val_usd'].sum()
    
    # buy_vol_agg e sell_vol_agg = APENAS BALEIAS > $500
    df_whales = df[df['val_usd'] >= WHALE_THRESHOLD].copy()
    res['buy_vol_agg'] = df_whales[df_whales['is_buy']].groupby('ts_bucket')['qty'].sum()
    res['sell_vol_agg'] = df_whales[df_whales['is_buy'] == False].groupby('ts_bucket')['qty'].sum()
    
    res = res.fillna(0)
    res['total_vol_agg'] = res['buy_vol_agg'] + res['sell_vol_agg']
    res['delta'] = res['buy_vol_agg'] - res['sell_vol_agg']
    res['cum_delta'] = res['delta'].cumsum()
    res['close_time'] = res.index + (15 * 60 * 1000) - 1
    res['price_range'] = (res['high'] - res['low']) / (res['close'] + 1e-9)
    res['vpin'] = abs(res['buy_vol_agg'] - res['sell_vol_agg']) / (res['volume'] + 1e-9)
    res['absorcao'] = (res['buy_vol_agg'] - res['sell_vol_agg']) / (res['price_range'] + 1e-9)
    
    cols_ant = ["ts","open","high","low","close","volume","quote_volume","trades","taker_buy_base","taker_buy_quote","close_time","cum_delta","total_vol_agg","buy_vol_agg","sell_vol_agg","vpin","price_range","absorcao"]
    res = res.reset_index().rename(columns={'ts_bucket': 'ts'})
    return res[cols_ant]

def resample_data(df, tf):
    # LÃ³gica de reamostragem para 30m, 1h, 4h, 8h, 1d (Mantendo as 900 linhas)
    logic = {'open':'first','high':'max','low':'min','close':'last','volume':'sum','quote_volume':'sum','trades':'sum','taker_buy_base':'sum','taker_buy_quote':'sum','cum_delta':'last','total_vol_agg':'sum','buy_vol_agg':'sum','sell_vol_agg':'sum','vpin':'mean','price_range':'mean','absorcao':'mean'}
    df['dt'] = pd.to_datetime(df['ts'], unit='ms')
    res = df.set_index('dt').resample(tf).agg(logic).dropna()
    res['ts'] = res.index.view(np.int64) // 10**6
    res['close_time'] = res['ts'] + (pd.to_timedelta(tf).total_seconds()*1000) - 1
    return res
def train_models(df_tf):
    print(f"--- Treinando com {len(df_tf)} candles ---")
    # DNA 78 COLUNAS
    df_tf['ret1'] = df_tf['close'].pct_change()
    df_tf['vol_real'] = df_tf['ret1'].rolling(20).std()
    
    feat_cols = ["volume","quote_volume","trades","taker_buy_base","taker_buy_quote","cum_delta","total_vol_agg","buy_vol_agg","sell_vol_agg","vpin","price_range","absorcao","ret1","vol_real"]
    while len(feat_cols) < 78:
        c_name = f"feat_placeholder_{len(feat_cols)}"
        df_tf[c_name] = 0.0
        feat_cols.append(c_name)
    
    X = df_tf[feat_cols].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kmeans = KMeans(n_clusters=6, random_state=42).fit(X_scaled)
    # ... aqui segue a lÃ³gica de treino K1-K6 idÃªntica ao V27 ...
    # (Omitido aqui por espaÃ§o, mas no script completo vai tudo)
    return scaler, kmeans

def main():
    threading.Thread(target=start_server, daemon=True).start()
    # LÃ³gica de download, loop de dias, salvamento de CSVs e PKLs
    # Chamada de upload_catbox no final
    print("ðŸš€ PROCESSO INICIADO...")

if __name__ == "__main__":
    main()