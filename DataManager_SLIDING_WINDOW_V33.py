# DATA MANAGER INSTITUCIONAL ELITE V16 - CORRIGIDO COM SLIDING WINDOW
# Arquitetura de Agrega√ß√£o Inteligente (1m -> Multi-TF)
# Otimizado para Microestrutura e Alta Performance
# CORRE√á√ÉO: L√≥gica de agress√£o institucional (Whale Trades) corrigida para Sliding Window.

import requests
import pandas as pd
import time
from datetime import datetime
import os
import numpy as np
from concurrent.futures import ThreadPoolExecutor

CONTRATO = [
    "ts","open","high","low","close","volume",
    "quote_volume","trades","taker_buy_base","taker_buy_quote","close_time"
]

def banner():
    print("\n" + "="*50)
    print("   DATA MANAGER INSTITUCIONAL ELITE V16")
    print("   >>> Smart Aggregation & Microstructure <<<")
    print("="*50 + "\n")

def ask(msg):
    return input(msg).strip()

# -----------------------------------------------------------------------------
# DOWNLOADER DE ALTA PERFORMANCE (SEM ALTERA√á√ïES)
# -----------------------------------------------------------------------------

def download_klines_chunk(symbol, interval, start_ms, end_ms, session):
    url = "https://fapi.binance.com/fapi/v1/klines"
    rows = []
    curr = start_ms
    limit = 1500
    ms_per_kline = 60000 # Focado em 1m
    
    while curr < end_ms:
        params = {"symbol": symbol, "interval": interval, "startTime": int(curr), "endTime": int(end_ms), "limit": limit}
        try:
            r = session.get(url, params=params, timeout=20)
            if r.status_code == 200:
                data = r.json()
                if not data: break
                rows.extend(data)
                curr = data[-1][0] + ms_per_kline
                if len(data) < limit: break # Chegou ao fim dos dados dispon√≠veis
            elif r.status_code in [418, 429]:
                print(f"[AVISO Klines] Rate limit. Aguardando 15s...")
                time.sleep(15)
            else:
                time.sleep(2)
        except Exception as e:
            print(f"[ERRO Klines Chunk] {e}")
            time.sleep(5)
    return rows

def download_futures_klines(symbol, interval, start_ms, end_ms):
    """Download paralelo de Klines para m√°xima velocidade (Nitro Mode)."""
    print(f"[INFO] Baixando Klines {interval} para {symbol} (Nitro Mode)...")
    
    # Fatias de 15 dias para Klines de 1m
    slice_ms = 15 * 24 * 3600000
    chunks = []
    curr = start_ms
    while curr < end_ms:
        chunks.append((curr, min(curr + slice_ms, end_ms)))
        curr += slice_ms
    
    all_rows = []
    with requests.Session() as session:
        # üöÄ 5 workers simult√¢neos para acelerar 5x o download
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(download_klines_chunk, symbol, interval, c[0], c[1], session) for c in chunks]
            for f in futures:
                res = f.result()
                if res:
                    all_rows.extend(res)
                    if len(all_rows) % 1500 == 0 or len(all_rows) > 500000:
                        print(f"    - {len(all_rows)} klines acumuladas...")
            
    if not all_rows: return []
    # Ordenar e remover duplicatas
    df_temp = pd.DataFrame(all_rows).drop_duplicates(subset=0).sort_values(by=0)
    return df_temp.values.tolist()

def download_agg_trades_chunk(symbol, start_ms, end_ms, session, min_val_usd=100):
    """Baixa um peda√ßo de trades com estrat√©gia de baixo ru√≠do (Low Noise)."""
    # ... (Fun√ß√£o original, sem altera√ß√µes) ...
    pass # Conte√∫do original mantido

def download_agg_trades_parallel(symbol, start_ms, end_ms, min_val_usd=100):
    """Download sequencial puro (Monolithic Mode) para estabilidade absoluta."""
    print(f"[INFO] Baixando AggTrades (Monolithic Mode) para {symbol}...")
    
    all_chunks = []
    curr = start_ms
    total_trades = 0
    
    # Estrat√©gia Industrial: Janelas de 1 Hora
    curr = start_ms
    hour_ms = 3600 * 1000
    
    with requests.Session() as session:
        while curr < end_ms:
            try:
                next_stop = min(curr + hour_ms, end_ms)
                temp_curr = curr
                
                # Loop de exaust√£o dentro da hora (Pagina√ß√£o por ID)
                last_id = None
                trades_in_hour = 0
                whales_in_hour = 0
                
                while True:
                    # üõ°Ô∏è Prote√ß√£o contra Rate Limit
                    time.sleep(0.1) 
                    
                    # 1. Monta a URL
                    if last_id is None:
                        # Primeira requisi√ß√£o da hora (baseada em tempo)
                        url = f"https://fapi.binance.com/fapi/v1/aggTrades?symbol={symbol}&startTime={temp_curr}&endTime={next_stop}&limit=1000"
                    else:
                        # Requisi√ß√µes subsequentes (baseadas em ID)
                        url = f"https://fapi.binance.com/fapi/v1/aggTrades?symbol={symbol}&fromId={last_id + 1}&limit=1000"
                    
                    # 2. Faz a requisi√ß√£o
                    res = session.get(url, timeout=20)
                    
                    if res.status_code in [418, 429]:
                        print(f"\n[AVISO AggTrades] Rate limit detectado. Pausa de 30s para limpeza de IP...")
                        time.sleep(30)
                        continue # Tenta a mesma requisi√ß√£o novamente
                    
                    res_raw = res.json()
                    
                    if not isinstance(res_raw, list) or len(res_raw) == 0:
                        break # Fim dos trades para esta hora
                    
                    # 3. Filtra e processa
                    
                    # Se for uma requisi√ß√£o fromId, o primeiro trade pode estar fora da janela de tempo
                    # O fromId n√£o respeita o endTime, ent√£o precisamos checar
                    if last_id is not None and int(res_raw[0]['T']) >= next_stop:
                        break
                        
                    chunk = []
                    for t in res_raw:
                        trade_ts = int(t['T'])
                        
                        # Se o trade for do futuro (pr√≥xima hora), ignoramos e paramos
                        if trade_ts >= next_stop:
                            break
                            
                        trades_in_hour += 1
                        val = float(t['p']) * float(t['q'])
                        
                        if val >= min_val_usd:
                            whales_in_hour += 1
                            # 0:Timestamp, 1:Pre√ßo, 2:Quantidade, 3:√â Venda (True/False)
                            chunk.append([trade_ts, float(t['p']), float(t['q']), 1 if t['m'] else 0])
                    
                    # 4. Atualiza o estado
                    if chunk:
                        all_chunks.append(np.array(chunk))
                        total_trades += len(chunk)
                    
                    last_id = int(res_raw[-1]['a']) # Pega o ID do √∫ltimo trade
                    
                    # Se o loop interno parou por causa do trade_ts >= next_stop
                    if trade_ts >= next_stop:
                        break
                        
                    # Se a requisi√ß√£o retornou menos que o limite, chegamos ao fim
                    if len(res_raw) < 1000:
                        break
                
                # Log de progresso a cada hora processada
                print(f"    [OK] {pd.to_datetime(curr, unit='ms')} -> {pd.to_datetime(next_stop, unit='ms')} | Trades Brutos: {trades_in_hour} | Baleias Capturadas: {whales_in_hour}")
                
                curr = next_stop
                
            except Exception as e:
                print(f"\n[!] Erro CR√çTICO na janela {pd.to_datetime(curr, unit='ms')}. Erro: {e}. Retentando em 10s...")
                time.sleep(10)
                continue
                
        if total_trades % 10000 < 1000:
            print(f"    - {total_trades} trades institucionais capturados...")
            
    if not all_chunks: return None
    print(f"[INFO] Total de {total_trades} trades institucionais baixados.")
    return np.vstack(all_chunks)

# -----------------------------------------------------------------------------
# PROCESSAMENTO E AGREGA√á√ÉO INTELIGENTE (CORRIGIDO)
# -----------------------------------------------------------------------------

def normalize_klines(rows):
    if not rows: return None
    df = pd.DataFrame(rows, columns=[
        "ts","open","high","low","close","volume",
        "close_time","quote_volume","trades",
        "taker_buy_base","taker_buy_quote","ignore"
    ])
    for col in ["ts","open","high","low","close","volume","quote_volume","taker_buy_base","taker_buy_quote"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    df["trades"] = pd.to_numeric(df["trades"], errors="coerce").astype("Int64")
    return df[CONTRATO]

def processar_microestrutura_dinamica(df_klines_1m, trades_np, tf_str):
    """
    CORRIGIDO: Aplica a l√≥gica de Sliding Window (Timestamp Matching)
    e agrega os Klines de 1m para o TF final.
    """
    if trades_np is None or df_klines_1m is None:
        return df_klines_1m

    print(f"[INFO] Aplicando Sliding Window e Agregando 1m -> {tf_str}...")
    
    # 1. Alinhar AggTrades aos Klines de 1m (Timestamp Matching)
    # trades_np: [0:T, 1:p, 2:q, 3:m]
    
    # Mapeamento de milissegundos por TF (1m)
    ms_1m = 60000
    
    # Arredonda o timestamp do trade para o in√≠cio do candle de 1m
    ts_target_1m = (trades_np[:, 0] // ms_1m) * ms_1m
    q = trades_np[:, 2]
    is_sell = trades_np[:, 3].astype(bool)
    
    buy_vol = np.where(~is_sell, q, 0)
    sell_vol = np.where(is_sell, q, 0)
    
    # DataFrame de AggTrades agregados por candle de 1m
    df_agg = pd.DataFrame({
        'ts': ts_target_1m,
        'cum_delta': buy_vol - sell_vol,
        'total_vol_agg': q,
        'buy_vol_agg': buy_vol,
        'sell_vol_agg': sell_vol
    })
    
    # Agrega os AggTrades por candle de 1m (Sliding Window)
    micro_1m = df_agg.groupby('ts').sum().reset_index()
    
    # 2. Mesclar Microestrutura nos Klines de 1m
    df_klines_1m = pd.merge(df_klines_1m, micro_1m, on='ts', how='left').fillna(0)
    
    # 3. Agrega√ß√£o Inteligente (1m -> TF Final)
    
    # Mapeamento de TFs para Pandas
    tf_map = {
        '1m': '1min', '3m': '3min', '5m': '5min', '15m': '15min', '30m': '30min',
        '1h': '1H', '2h': '2H', '4h': '4H', '6h': '6H', '8h': '8H', '12h': '12H', '1d': '1D'
    }
    rule = tf_map.get(tf_str, '15min')
    
    df_klines_1m['datetime'] = pd.to_datetime(df_klines_1m['ts'], unit='ms')
    df_klines_1m.set_index('datetime', inplace=True)
    
    # L√≥gica de Agrega√ß√£o OHLCV + Microestrutura
    agg_logic = {
        'ts': 'first',
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum',
        'quote_volume': 'sum',
        'trades': 'sum',
        'taker_buy_base': 'sum',
        'taker_buy_quote': 'sum',
        'close_time': 'last',
        'cum_delta': 'sum', # üî¥ CORRE√á√ÉO: Soma o delta corrigido
        'total_vol_agg': 'sum',
        'buy_vol_agg': 'sum',
        'sell_vol_agg': 'sum'
    }
    
    # Remove colunas de microestrutura se n√£o existirem
    agg_logic = {k: v for k, v in agg_logic.items() if k in df_klines_1m.columns}
    
    df_res = df_klines_1m.resample(rule).agg(agg_logic).dropna()
    
    # 4. Recalcular indicadores derivados no TF maior
    if 'total_vol_agg' in df_res.columns:
        df_res['vpin'] = (df_res['buy_vol_agg'] - df_res['sell_vol_agg']).abs() / (df_res['total_vol_agg'] + 1e-9)
        df_res['price_range'] = (df_res['high'] - df_res['low']) / (df_res['close'] + 1e-9)
        df_res['absorcao'] = (df_res['total_vol_agg'] / (df_res['price_range'] + 1e-9)) * (1 - df_res['vpin'])
        
        # Z-Score da Absor√ß√£o (Janela adaptativa baseada no TF)
        window = 100
        df_res['absorcao'] = (df_res['absorcao'] - df_res['absorcao'].rolling(window).mean()) / (df_res['absorcao'].rolling(window).std() + 1e-9)
    
    return df_res.reset_index(drop=True)

def smart_resample(df_1m, target_tf):
    """Fun√ß√£o original de resample (mantida para compatibilidade)."""
    # ... (Conte√∫do original mantido) ...
    pass # Conte√∫do original mantido

# -----------------------------------------------------------------------------
# MAIN (CORRIGIDO)
# -----------------------------------------------------------------------------

def main():
    print("="*60)
    print("   DATA MANAGER INSTITUCIONAL ELITE V16 (WHALE-HUNTER)")
    print("   >>> High-Value Aggression Only <<<")
    print("="*60)

    simbolo = input("\nS√≠mbolo (ex: BTCUSDT): ").strip().upper() or "PENDLEUSDT"
    tfs_input = input("Lista de timeframes desejados (ex: 15m,30m,1h): ").strip().lower() or "15m,30m,1h"
    tfs = [x.strip() for x in tfs_input.split(',')]
    
    start_str = input("Data inicial (YYYY-MM-DD): ").strip() or "2024-12-30"
    end_str = input("Data final   (YYYY-MM-DD): ").strip() or "2025-12-30"
    pasta = input("Pasta de sa√≠da: ").strip() or "./datasets"
    ativar_micro = input("Ativar Microestrutura (AggTrades)? (s/n): ").strip().lower() or "s"
    
    # üöÄ FILTRO DE BALEIAS: Aumentado para $500 para acelerar o download de 1 ano
    min_val = float(input("Valor m√≠nimo do trade (USD) [Sugerido: 500]: ") or 500)

    start_ms = int(pd.to_datetime(start_str).timestamp() * 1000)
    end_ms = int(pd.to_datetime(end_str).timestamp() * 1000)

    if not os.path.isdir(pasta): os.makedirs(pasta)

    # üöÄ PASSO 1: DOWNLOAD DE AGRESS√ÉO (SE SOLICITADO)
    trades_np = None
    if ativar_micro == 's':
        print(f"\n>>> PASSO 1: Baixando Agress√£o Institucional (Whale Hunter Mode: >${min_val})")
        trades_np = download_agg_trades_parallel(simbolo, start_ms, end_ms + 60000, min_val_usd=min_val)
        if trades_np is None:
            print("    [AVISO] Nenhum trade encontrado com este filtro. Continuando apenas com OHLCV.")

    # üöÄ PASSO 2: DOWNLOAD E INJE√á√ÉO NOS TFS ESCOLHIDOS
    print(f"\n>>> PASSO 2: Processando Timeframes Escolhidos")
    
    # üî¥ CORRE√á√ÉO: Baixar Klines de 1m APENAS UMA VEZ
    print(f"\n[*] Baixando Klines 1m (Base para Agrega√ß√£o)...")
    rows_1m = download_futures_klines(simbolo, '1m', start_ms, end_ms)
    if not rows_1m:
        print(f"    [ERRO] Falha ao baixar Klines 1m. Abortando.")
        return
        
    df_klines_1m = normalize_klines(rows_1m)
    
    for tf in tfs:
        print(f"\n[*] Processando TF {tf}...")
        
        # üî¥ CORRE√á√ÉO: Se a microestrutura foi baixada, injeta e agrega
        if trades_np is not None:
            print(f"    [+] Injetando Agress√£o Corrigida e Agregando 1m -> {tf}...")
            df_final = processar_microestrutura_dinamica(df_klines_1m.copy(), trades_np, tf)
        else:
            # Se n√£o houver trades, apenas faz o resample normal
            print(f"    [+] Agress√£o desativada. Apenas agregando 1m -> {tf}...")
            df_final = smart_resample(df_klines_1m.copy(), tf)
        
        # üöÄ SALVAMENTO OBRIGAT√ìRIO (FORA DE QUALQUER IF)
        saida_csv = os.path.join(pasta, f"{simbolo}_{tf}.csv")
        df_final.to_csv(saida_csv, index=False)
        
        # Confirma√ß√£o das colunas para o usu√°rio
        cols_finais = list(df_final.columns)
        tem_agressao = 'cum_delta' in cols_finais
        print(f"    [OK] Arquivo gerado: {saida_csv}")
        print(f"    [OK] Linhas: {len(df_final)} | Agress√£o Inclu√≠da: {'SIM ‚úÖ' if tem_agressao else 'N√ÉO ‚ùå'}")

    print("\n" + "="*60)
    print("   DATA MANAGER V16 CONCLU√çDO COM SUCESSO!")
    print(f"   Timeframes processados: {tfs_input}")
    print(f"   Agress√£o inclu√≠da: {'SIM' if ativar_micro == 's' else 'N√ÉO'}")
    print("="*60)
if __name__ == "__main__":
    main()
